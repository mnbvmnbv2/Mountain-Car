{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593313f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  2\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set seed\n",
    "seed = 1453\n",
    "\n",
    "problem = \"MountainCarContinuous-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "# This is needed to get the input size for the NN\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "# This is needed to clip the actions within the legal boundaries\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a25ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a noise used in the paper that introduced DDPG https://arxiv.org/pdf/1509.02971v6.pdf\n",
    "# From what I understand, we can also use Gaussian or other noise without much difference,\n",
    "# I left the noise as is\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(layer1=400, layer2=300):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic(layer1=400, layer2=300):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Make it into a keras model\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its important to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor_model = get_actor(layer1=400, layer2=300)\n",
    "        self.critic_model = get_critic(layer1=400, layer2=300)\n",
    "\n",
    "        self.target_actor = get_actor(layer1=400, layer2=300)\n",
    "        self.target_critic = get_critic(layer1=400, layer2=300)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "        \n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "    \n",
    "    # Move the update and learn function from buffer to Agent to \"decrease\" scope\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object, use_noise=True, noise_mult=1):\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:     \n",
    "            sampled_actions = tf.squeeze(self.actor_model(state)).numpy()\n",
    "            legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]\n",
    "        else:\n",
    "            sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "            noise = noise_object()\n",
    "            # Adding noise to action\n",
    "            sampled_actions = sampled_actions.numpy() + noise * noise_mult\n",
    "\n",
    "            # We make sure action is within bounds\n",
    "            legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f522c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed(x, episode):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=3, total_episodes=100, use_curve=False, curve_scale=1, use_momentum=False, \n",
    "            buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002, render=False,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005, noise_mult=1, save_weights=False, \n",
    "            directory='Weights/', actor_name='car_actor', critic_name='car_critic',\n",
    "            gamma_func=fixed, tau_func=fixed, critic_lr_func=fixed, actor_lr_func=fixed,\n",
    "            noise_mult_func=fixed, std_dev_func=fixed, mean_number=40):\n",
    "    # To store reward history of each episode\n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "\n",
    "        # add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        \n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(buffer_capacity=buffer_capacity, batch_size=batch_size, std_dev=std_dev, \n",
    "                critic_lr=critic_lr, actor_lr=actor_lr, gamma=gamma, tau=tau)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # functions for different parameters\n",
    "            agent.gamma = gamma_func(gamma, ep)\n",
    "            agent.tau = tau_func(tau, ep)\n",
    "            agent.critic_lr = critic_lr_func(critic_lr, ep)\n",
    "            agent.actor_lr = actor_lr_func(actor_lr, ep)\n",
    "            agent.noise_mult = noise_mult_func(noise_mult, ep)\n",
    "            agent.std_dev = std_dev_func(std_dev, ep)\n",
    "            \n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                \n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                action = agent.policy(tf_prev_state, agent.ou_noise, noise_mult=noise_mult)\n",
    "                # Recieve state and reward from environment.\n",
    "                state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                if use_curve:\n",
    "                    # https://medium.com/reinforcement-learning-w-policy-gradients/mountaincarcontinous-cheating-8446b09647ba\n",
    "                    # true_state = np.abs(np.cos(np.pi/3.) + state[0])\n",
    "                    # reward += -(1. - true_state)\n",
    "                    \n",
    "                    # I am a bit unsure how the curve is calculated (it just says sinusoidal on the webpage)\n",
    "                    # I base my attempt at the bottom being -0.5 and top right being 0.5\n",
    "                    # so the below code is my attempt at this type of reward\n",
    "                    reward += (np.sin(state[0] * np.pi) + 1)/curve_scale\n",
    "                    # it adds 0 at the bottom and 2 at the top of the curve\n",
    "                    # adds curve scale as the reward could be too high\n",
    "                    \n",
    "                if use_momentum:\n",
    "                    # having 0 speed and being in 0,5 (which i guess is the bottom) should incur max penalty\n",
    "                    # having max speed and beging in 0,5 should not give any penalty\n",
    "                    # the rest of the penalties should be smooth around the bottom with regards to the speed\n",
    "                    reward -= max(0, np.cos(state[1]*np.pi*10))*max(0, np.cos((state[0]+0.5)*np.pi*2))\n",
    "                    #this version gives penalty for 0.05 or lower speeds in ca. range -0.7 to -0.3\n",
    "\n",
    "                agent.record((prev_state, action, reward, state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                # End this episode if en episode is done\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            \n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-mean_number:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "\n",
    "            # Mean of last x episodes\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-mean_number:])\n",
    "            print(\"Episode * {} * Avg Reward is ==> {} * true_avg_reward: {} * time used: {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, (time.time() - before)))\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "\n",
    "        if save_weights:\n",
    "            agent.actor_model.save_weights(directory + actor_name + '-trial' + str(trial) + '.h5')\n",
    "            agent.critic_model.save_weights(directory + critic_name + '-trial' + str(trial) + '.h5')\n",
    "    \n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (\" + str(mean_number) + \")\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return to be able to make graphs etc. later, or use the data for other stuff\n",
    "    return true_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57bcf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(total_episodes=10, actor_weights='Weights/car_actor-trial0.h5', render=False):\n",
    "    rewards = []\n",
    "    \n",
    "    for ep in range(total_episodes):\n",
    "        ep_reward = 0\n",
    "        \n",
    "        # Used for time benchmarking\n",
    "        before = time.time()\n",
    "        \n",
    "        prev_state = env.reset()\n",
    "        agent = Agent(buffer_capacity=0, batch_size=0, std_dev=0, \n",
    "                critic_lr=0, actor_lr=0, gamma=0, tau=0)\n",
    "        agent.actor_model.load_weights(actor_weights)\n",
    "        \n",
    "        while True:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "            action = agent.policy(tf_prev_state, 0, use_noise=False)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            ep_reward += reward\n",
    "\n",
    "            if done:\n",
    "                print(str(time.time() - before) + 's')\n",
    "                rewards.append(ep_reward)\n",
    "                break\n",
    "\n",
    "            prev_state = state\n",
    "            \n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True reward\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83b8ba",
   "metadata": {},
   "source": [
    "---\n",
    "# Runs and tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b912044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a(b, ep):\n",
    "    return b-ep/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0070c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(total_trials=1, total_episodes=10, gamma_func=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4b7de82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -558.7951604284518 * true_avg_reward: -54.265199540551066 * time used: 8.910804271697998\n",
      "Episode * 1 * Avg Reward is ==> -492.41790608985104 * true_avg_reward: -52.83849626424455 * time used: 8.44083046913147\n",
      "Episode * 2 * Avg Reward is ==> -461.539933396388 * true_avg_reward: -61.24735543818452 * time used: 8.154120206832886\n",
      "Episode * 3 * Avg Reward is ==> -448.14792428228697 * true_avg_reward: -68.50932729908067 * time used: 7.728546380996704\n",
      "Episode * 4 * Avg Reward is ==> -430.3947808805589 * true_avg_reward: -71.95520700055685 * time used: 8.213915824890137\n",
      "Episode * 5 * Avg Reward is ==> -399.19688467196596 * true_avg_reward: -72.17460987184295 * time used: 8.68928575515747\n",
      "Episode * 6 * Avg Reward is ==> -352.5077653159782 * true_avg_reward: -53.71858839579528 * time used: 4.55070424079895\n",
      "Episode * 7 * Avg Reward is ==> -325.7550390795865 * true_avg_reward: -54.592790203683236 * time used: 8.58939003944397\n",
      "Episode * 8 * Avg Reward is ==> -306.8069153238631 * true_avg_reward: -55.1726267600019 * time used: 8.569276332855225\n",
      "Episode * 9 * Avg Reward is ==> -269.02784607928123 * true_avg_reward: -40.87659925946635 * time used: 1.586381435394287\n",
      "Episode * 10 * Avg Reward is ==> -238.06885589123533 * true_avg_reward: -28.88092720081572 * time used: 0.8760907649993896\n",
      "Episode * 11 * Avg Reward is ==> -212.0349782085934 * true_avg_reward: -18.68873712584185 * time used: 0.7848474979400635\n",
      "Episode * 12 * Avg Reward is ==> -189.69428239771744 * true_avg_reward: -10.0695554982755 * time used: 0.8234760761260986\n",
      "Episode * 13 * Avg Reward is ==> -170.7517263577019 * true_avg_reward: -2.686418446798804 * time used: 0.8092622756958008\n",
      "Episode * 14 * Avg Reward is ==> -154.8272302303996 * true_avg_reward: 3.6618915682202613 * time used: 0.8587639331817627\n",
      "Episode * 15 * Avg Reward is ==> -141.32869391333003 * true_avg_reward: 9.180216621970942 * time used: 0.9453921318054199\n",
      "Episode * 16 * Avg Reward is ==> -128.73416461598705 * true_avg_reward: 14.016353834745486 * time used: 0.9448838233947754\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEGCAYAAACO8lkDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtlElEQVR4nO3dd3hUZdrH8e+dnkBI6EhCb0ovoYgVG66rYq8giGBDXXXV1d13176yupZ1ETsCgiBiQ117RaUldJAOCaGGmkhIv98/5kQDaROSyZnM3J/rmmvmnJPM+YWSe55zniKqijHGGFNSiNsBjDHG+B8rDsYYY0qx4mCMMaYUKw7GGGNKseJgjDGmlDC3A9SEJk2aaNu2bd2OYYwxdUpKSsoeVW1a1rGAKA5t27YlOTnZ7RjGGFOniEhqecfsspIxxphSrDgYY4wpxYqDMcaYUgLinkNZ8vPzSU9PJycnx+0o5YqKiiIxMZHw8HC3oxhjzBECtjikp6cTGxtL27ZtERG345Siquzdu5f09HTatWvndhxjjDlCwF5WysnJoXHjxn5ZGABEhMaNG/t1y8YYE7wCtjgAflsYivl7PmNM8Aro4mCMMYEqt6CQF7/byOK0/T55fysOPvbZZ5/RpUsXOnbsyPjx492OY4yp41SVr3/ZxdBnf+Bfn63hi1W7fHKegL0h7Q8KCwsZN24cX375JYmJifTv358LL7yQrl27uh3NGFMHbcz4lUc+Ws336zJo37QeU0YP4LTOZc5+UW1WHHxo4cKFdOzYkfbt2wNw1VVX8eGHH1pxMMZUSWZOPv/9ej1v/LSF6PBQ/u+PJ3DdiW2JCPPdxZ+gKA4Pf7SK1dsza/Q9u7ZswIMXdKvwa7Zt20arVq1+205MTGTBggU1msMYE7iKipTZKek8+fka9h7K44p+rbhnaBeaxkb6/NxBURyMMaauSUndz8MfrWJ5+kH6to7njVED6JEYV2vnD4riUNknfF9JSEhg69atv22np6eTkJDgShZjTN2wKzOHf326hveWbKN5g0ieu7I3w3q3rPWu70FRHNzSv39/1q9fz+bNm0lISGDmzJm89dZbbscyxvih3IJCXv9xMxO+2UBBoXLr6R0YN6Qj9SLd+TVtxcGHwsLCmDBhAkOHDqWwsJDRo0fTrZs7rRhjjH/ydE3dzaOfrCZ1bzZnndCcv59/Am0a13M1lxUHHzvvvPM477zz3I5hjPFDG3b/yiMfr+aHdRl0aFqPqaMHcKqPuqZWlRUHY4ypZZk5+Tz/1Xom/+zpmvr387ty3YltCA/1n3HJVhyMMaaWqCrvpKTz5GeerqlXJnm6pjap7/uuqVUV0MVBVf16cjtVdTuCMaaWpO/P5oH3VjB3/R76tWlY611Tqypgi0NUVBR79+7122m7i9dziIqKcjuKMcaHVJUZC7fyz//9gqry2EXduWZAa0JC/O/3UkkBWxwSExNJT08nIyPD7SjlKl4JzhgTmNL3Z3P/uyv4ccMeTurYmPGX9KRVoxi3Y3klYItDeHi4rbBmjHGFqjJ9QRpP/O8XAB6/2NNa8MerGOUJ2OJgjDFu2Lovm7+8u5yfN+7l5I5NGH9pDxIb1o3WQklWHIwxpgYUFSnTF3paCwL88+IeXD2gVZ1qLZRkxcEYY6qpZGvhlE5NeOKSutlaKMmKgzHGHKOiImX6glSe+HQNISI8cUkPrupfd1sLJVlxMMaYY7B1Xzb3zV7OvE2e1sL4S3uSEB/tdqwaY8XBGGOqoKhImbYglfGfriFUhH9d2oMrkgKjtVCSFQdjjPFS2t5s7p29jAWb93Fq56aMv6QHLQOotVCSFQdjjKlEUZHy5nxPayEsJHBbCyVZcTDGmArsyszhTzOXMH/TPk7r3JQnAri1UFKlxUFEQoBeQEvgMLBSVXf7Opgxxrjtpw17+NPMJWTnFQZFa6GkcouDiHQA/gKcBawHMoAooLOIZAMvA1NUtag2ghpjTG0pKlImfreBZ75cR/um9Zkxti+dmse6HatWVdRyeAx4EbhJj5pbWkSaAdcAI4ApvotnjDG1a/+hPO6atZTv1mYwrHdL/nlxD9fWcXZTuT+xql5dwbHdwHO+CGSMMW5ZuvUA46YvJiMrl0cv6s7wgXVrsryaVGE5FJHjgWFAgrNrGzBHVX/xdTBjjKktqsrUeak89slqmsVGMfuWE+mZGO92LFeVu2CpiPwFmAkIsNB5CDBDRO73dTAROVdE1orIhto4nzEmOP2aW8AdM5fy4JxVnNKpKZ/ccXLQFwaouOVwA9BNVfNL7hSRZ4BVwHhfhRKRUOAF4GwgHVgkInNUdbWvzmmMCT7rdmVxy7QUNu85xH3nduHmUzv4/QpttaWi4lCEp/tq6lH7j3OO+dIAYIOqbgIQkZl4Lm9ZcTDG1Ij3l6Tz1/dWUi8yjOljBnFih8ZuR/IrFRWHO4GvRWQ9sNXZ1xroCNzm41wJJc4JntbDwJJfICI3AjcCtG7d2sdxjDGBIie/kEc+Xs1bC9IY0K4RE67uQ7MGtpb70SrqrfSZiHTG8ym+5A3pRapaWBvhKqKqrwCvACQlJWklX26MMWzdl80t01NYuS2Tm0/rwD3ndCYstNxbr0Gtwt5KzgC3+cXbInKrqs6v4FtqyjagVYntRGefMcYck69W7+LuWUtR4NXrkji7a3O3I/m1ikZI313G7r+KSBSAqj7js1SwCOgkIu3wFIWr8Ay6M8aYKikoLOLfX6zjpe830j2hAROv6UfrxnV7lbbaUFHL4WHgf3h6JhXfvg8FfD6GXFULROQ24HPnnJNUdZWvz2uMCSy7M3O4bcYSFm7exzUDW/OP87sSFR7qdqw6oaLi0A14GqgHPKyq2SIyUlUfro1gqvo/PMXJGGOqbN7Gvdw+YwmHcgt45opeXNI30e1IdUq5d2JUNU1VLwd+Br4UkctqL5Yxxhy7WclbGf76AhpEh/HBuJOsMByDSm/Tq+qHwDl4upKm+zyRMcYcI1Xl+a/Xc9/s5Qzu0JgPx51ElxbBNZtqTfFqqkFVPQTc6+MsxhhzzAoKi/j7h6uYsTCNS/omMP6SnkSEWTfVY1XR3EoficgFIhJexrH2IvKIiIz2bTxjjKnc4bxCbp6WwoyFaYwb0oGnL+9lhaGaKmo5jAXuBp4TkX38vthPW2AjMMG55GSMMa7ZdyiP0ZMXsSz9AI8O68aIE9u6HSkgVDRCeidwH3CfiLTFM6fSYWCdqmbXTjxjjClf2t5sRr6xkO0HDvPitf04t3sLtyMFDG/vOWwBtvg0iTHGVMHKbQcZ9cYi8guLmD5mIEltG7kdKaAE39p3xpg67/t1Gdw6LYX4mAhm3jiQjs2sR1JNs+JgjKlTZqekc/+7y+nUPJbJ1/enuc2o6hNWHIwxdYKqMvG7jTz1+VpO6tiYl4b3IzaqVGdKU0MqmnhvBVDuVNiq2tMniYwx5iiFRcqDc1YybX4aF/VuyZOXWVdVX6uo5XC+8zzOeX7Teb7Wd3GMMeZIOfmF3DFjCV+s3sXNp3XgvqFdbCnPWlBRV9ZUABE5W1X7lDh0v4gsBu73dThjTHDbfyiPMVOTWZy2n4cu6Mqok9q5HSloeHPPQUTkJFX9ydkYjBdzMhljTHVs3ecZw5C+/zATr+nLH3oc53akoOJNcRgNvCEicc72AWefMcb4xMptB7l+8iJy8wuZdsNABrSzMQy1rcLiICKhwGmq2qu4OKjqwVpJZowJSnPXZ3DLtMU0iArjrVsG06m5jWFwQ4WXh1S1ELjaeX3QCoMxxpfeX5LO9W8sIrFhNO/depIVBhd5c1npJxGZALwNHCreqaqLfZbKGBN03kneyr2zl3Ni+8a8fF0/GtgYBld5Uxx6O8+PlNinwBk1nsYYE5TmLNvOX95dzimdmvDqdUm2zrMfqLQ4qOqQ2ghijAlOn63cyV1vLyWpbSNeGWGFwV94NX2GiPwR6IZnPQcAVPWR8r/DGGMq993a3dw+YzE9E+OYNKo/0RFWGPxFpeMVROQl4ErgdkCAy4E2Ps5ljAlwP2/Yw01vptC5eSyTrx9A/Uib6s2feDOYbbCqXgfsV9WHgROBzr6NZYwJZMlb9jFmajJtG9fjzRsGEhdtN5/9jTfF4bDznC0iLYF8PKvCGWNMlS1PP8D1byyiRYMo3hwzgEb1ItyOZMrgTTvuYxGJB54CFuPpqfSqL0MZYwLTLzsyGfH6QuLrhTN97ECaxdpaDP7Km95Kjzov3xWRj4EoGwxnjKmqDbt/ZfhrC4iJCOWtMYM4Li7a7UimApUWBxH5EfgemAv8ZIXBGFNVqXsPce1r8xERpo8ZSKtGMW5HMpXw5p7DCGAtcCnws4gki8izvo1ljAkU2w4c5ppXF5BXUMT0MQNp37S+25GMF7y5rLRZRHKAPOcxBDjB18GMMXXfrswcrnl1Ppk5+cwYO4guLWyupLrCm3EOG4EPgObA60B3VT3Xx7mMMXXcnl9zufa1BezJymXK6AF0T4ir/JuM3/DmstLzQBqe2VnvAEaKSAefpjLG1GkHsvMY8fpC0vdn8/qo/vRt3dDtSKaKKi0OqvofVb0cOAtIAR4C1vk4lzGmjsrKyWfkpIVs3P0rr4xIYlD7xm5HMsfAm95KTwMnA/WBn4F/4Om5ZIwxR8jOK2D05EWs2p7JS8P7cWrnpm5HMsfIm0Fw84AnVXWXr8MYY+qunPxCxkxJJiV1P/+9ui9ndW3udiRTDd7cc3gPOFtE/g4gIq1FZIBvYxlj6pK8giJumZbCvE17efqKXvyxp82wU9d5UxxewDPZ3jXOdpazzxhjKCgs4o4ZS/h2bQaPX9SDi/skuh3J1ABvLisNVNW+IrIEQFX3i4jNlGWMoahI+fM7y/hs1U4evKAr1wxs7XYkU0O8aTnki0gongn3EJGmQFF1TioiT4nIGhFZLiLvOxP7FR97QEQ2iMhaERlanfMYY3zrua/X8+HS7dw7tAvXn9TO7TimBnk7zuF9oJmIPA78CDxRzfN+iWcwXU883WIfABCRrsBVeFadOxeY6BQmY4yf+XTFDp7/ej2X90vk1tNt6FOg8Wb6jOkikgKciWcluIvwDIo7Zqr6RYnN+cBlzuthwExVzQU2i8gGYACeHlPGGD/xy45M7p61jD6t43ns4u6IiNuRTA2rsOUgIgkikgRsUtUXgFl4JuJbX4MZRgOfOq8TgK0ljqU7+8rKdqMzCWByRkZGDcYxxlRk36E8xk5NpkF0GC8P70dkmDXuA1G5xUFE7gSWAv8F5ovIGOAXIBroV9kbi8hXIrKyjMewEl/zN6AAmF7V4Kr6iqomqWpS06Y20MaY2pBfWMS46YvZnZXLKyOSaNbAFusJVBVdVroR6KKq+0SkNZ57Ayepaoo3b6yqZ1V0XERGAecDZ6qqOru3Aa1KfFmis88Y4wce+3g18zbt5ZkretGrVbzbcYwPVXRZKUdV9wGoahqw1tvCUBkRORe4D7hQVbNLHJoDXCUikSLSDugELKyJcxpjqmfmwjSmzEtl7CntuKSvjWUIdBW1HBJF5PkS28eV3FbVO6px3glAJPClcyNrvqrerKqrRGQWsBrP5aZxqlpYjfMYY2pASuo+/v7hSk7p1IS/nHu823FMLaioONx71HaNtBoAVLVjBcceBx6vqXMZY6pn+4HD3PTmYhLio5lwdV/CQr3pAW/qunKLg6pOqc0gxhj/k5NfyE1vppCTX8iMsQOJiwl3O5KpJd5Mn2GMCUKqyv3vLmfl9oO8OiKJTs1tic9gYu1DY0yZXp27iQ+Wbueec7rY9NtByIqDMaaU79buZvyna/hjj+NsaowgVWlxEJEpR02M11BEJvk0lTHGNZsyfuX2GUs4vkUDnrq8p02NEaS8aTn0VNUDxRuquh/o47NExhjXZObkM3ZqMuGhIbxyXT9iIuy2ZLDypjiEiEjD4g0RaYTdyDYm4BQWKXfOXErq3mwmXtuXxIYxbkcyLvLml/zTwDwReQfPrKyXYeMQjAk4T3+xlm/W7ObRi7ozqH1jt+MYl3kzZfdUEUkGznB2XaKqq30byxhTmz5atp2J323kmoGtGTGojdtxjB8otziISANVzXQuI+0E3ipxrFHxvEvGmLpt5baD3Dt7Gf3bNuShC7q5Hcf4iYpaDm/hmTU1BWeJUIc42+19mMsYUwv2/JrLjVOTaRQTwYvD+xERZr3bjUdF02ec7zzbwrDGBKC8giJunbaYfdl5zL55ME3qR7odyfiRii4r9a3oG1V1cc3HMcbUloc+WsXCLft4/uo+dE+IczuO8TMVXVZ62nmOApKAZXguKfUEkoETfRvNGOMr0+an8taCNG45vQMX9mrpdhzjh8q9wKiqQ1R1CLAD6OssydkPzwA4W53NmDpqRfpBHpqzijOOb8Y953RxO47xU97cfeqiqiuKN1R1JXCC7yIZY3wlJ7+QO99eQpP6kTx7RW9CQ2xqDFM2bwbBLReR14Bpzva1wHLfRTLG+Mq/PlvDxoxDTLvB1mYwFfOmOFwP3AL8ydn+AXjRZ4mMMT7x4/o9vPHTFkYNbsvJnZq4Hcf4OW9GSOeIyAvAV3jGN6xV1XyfJzPG1JiDh/O5d/Yy2jetZ2tAG69UWhxE5HRgCrAFT2+lViIyUlV/8GkyY0yNefDDlezOyuW9WwYTHRHqdhxTB3g78d45qroWQEQ6AzOAfr4MZoypGZ8s38EHS7dz51md6NUq3u04po7wprdSeHFhAFDVdYDdyTKmDtidmcPfPlhBr8Q4xg3p6HYcU4d403JILqO3UrLvIhljaoKqct+7y8nJL+SZK3sTHmrzJhnveVMcbgHGAXc423OBiT5LZIypEdMXpPHd2gwevrAbHZrWdzuOqWO86a2UCzzjPIwxdcDmPYd4/JNfOKVTE1ufwRyTiibem6WqV4jICo6cshsAVe3p02TGmGNSUFjE3bOWEh4qPHVZL0JsFLQ5BhW1HIoHvZ1fG0GMMTXjpe83siTtAP+5qjct4qLcjmPqqIom3tvhvNwDbFXVVCAS6AVsr4VsxpgqWrntIM99tZ7zex7HsN4JbscxdZg33Rd+AKJEJAH4AhgBTPZlKGNM1eXkF3LX20tpXD+Cxy7q7nYcU8d5UxxEVbOBS4CJqno5YAvNGuNnnvp8Let3/8qTl/UiPibC7TimjvOqOIjIiXjGN3zi7LPx98b4kZ837uH1HzczYlAbTuvc1O04JgB4UxzuBB4A3lfVVSLSHvjWp6mMMV7LzMnnnlnLaNekHg+cZ5PqmZrhzTiH74HvRaSBiMSq6iZ+HxBnjHHZQ3NWsSsrl9k3n0hMhDfjWo2pXKUtBxFJcsY6LAdWisgyEbFJ94zxA5+t3MF7i7cx7vQO9Gnd0O04JoB48zFjEnCrqs4FEJGTgTcAGwRnjIt2Z+XwwHsr6JEQx+1ndnI7jgkw3txzKCwuDACq+iNQ4LtIxpjKqCr3v7uC7LxCnr2yl02qZ2qcNy2H70XkZTxrOChwJfCdiPQFUNXFPsxnjCnDzEVb+WbNbv5xflc6Not1O44JQN4Uh17O84NH7e+Dp1iccawnF5E/A/8GmqrqHhER4D/AeUA2MMqKjzFHSt17iEc/Xs1JHRszanBbt+OYAOVNb6UhvjixiLQCzgHSSuz+A9DJeQwEXnSejTFAYZHy51nLCA2xSfWMb5V7oVJEnivx+k9HHZtcA+d+FriPI2d8HQZMVY/5QLyIHFcD5zImILz8w0aSU/fzyLButIyPdjuOCWAV3cU6tcTrkUcdq1ZPJREZBmxT1WVHHUoAtpbYTnf2lfUeN4pIsogkZ2RkVCeOMXXCqu0HefbLdZzXowUX2aR6xscquqwk5bz2ioh8BbQo49DfgL/iuaR0zFT1FeAVgKSkpFLrTRgTSHILCrn77WXEx0Tw+EU98NyeM8Z3KioOISLSEE/rovh18b/ISudWUtWzytovIj2AdsAy5x94IrBYRAYA24BWJb480dlnTFCb/NMW1u7K4vWRSTSsZ5PqGd+rqDjEASn8XhBK9ho65k/qqroCaFa8LSJbgCSnt9Ic4DYRmYnnRvTBEutKGBOUdmfl8N9vNnDm8c0484TmbscxQaLc4qCqbWsxR7H/4enGugFPV9brXchgjF956rO15BYU8n/nd3U7igkirs/SVbIIqaoC49xLY4x/Wbb1AO+kpHPTqe1p16Se23FMELEx98b4KVXloY9W0aR+JLed0dHtOCbIWHEwxk99sHQbS9IOcN+5XYiNCnc7jgkyVhyM8UOHcgsY/+kaeibGcVnfRLfjmCB0TMVBRD6u6SDGmN9N/G4DuzJzefCCbjZFhnHFsbYcxtZoCmPMb9L2ZvPq3M1c3CeBfm1sAR/jjmMqDjb2wBjfefx/qwkLEf5yrq0HbdxTaVdWZ4nQowe9HQSSgcdUda8vghkTjH7asIfPV+3i3qFdaBEX5XYcE8S8GefwKVAIvOVsXwXEADuBycAFPklmTJApKCzikY9W06pRNDec3M7tOCbIeVMczlLVviW2V4jIYlXtKyLDfRXMmGDz1sI01u7K4qXh/YgKr3T6MmN8ypt7DqHOpHgAiEh/fp94z9aSNqYG7D+Ux9NfrGNwh8YM7WbzJxn3edNyGANMEpH6znYWcIOI1AOe8FkyY4LIs1+tIysnnwcv6GbTcRu/4E1xWKyqPUQkDkBVD5Y4Nss3sYwJHmt2ZjJtfirDB7WhS4tYt+MYA3h3WWmziLwCJAGZPs5jTFBRVR6es5oG0eHcfXZnt+MY8xtvisPxwFd4ZkvdLCITRORk38YyJjh8vmon8zbt5e6zOxMfY4v4GP9RaXFQ1WxVnaWqlwB9gAbA9z5PZkyAy8kv5LFPfqFL81iuGdDa7TjGHMGrEdIicpqITMSzMlwUcIVPUxkTBF6bu4n0/Yd58IKuhIXaHJjGv3gzQnoLsATPzed7VfWQr0MZE+h2HDzMC99u5NxuLRjcsYnbcYwpxZveSj1VNRNARDqIyDXAVarazbfRjAlc//p0DYWq/O2PJ7gdxZgyedOWrS8id4nIImCV8z1X+TaWMYErJXUfHyzdzo2ntKdVoxi34xhTpnKLg4jcKCLfAt8BjYEbgB2q+rCqrqilfMYElKIi5aE5q2nRIIpbh3RwO44x5arostIEYB5wjaomA4jI0bOzGmOqYHZKOiu2HeS5K3sTE+HNVV1j3FHRv87jgMuBp0WkBZ4b0raQrTHHKCsnnyc/X0O/Ng0Z1rul23GMqVC5l5VUda+qvqSqpwFnAgeAXSLyi4j8s7YC1hXZeQVkZOW6HcP4sf9+s4G9h/J48IKuNn+S8XtetWtVNR14Gk8rojNBfEM6r6CIzXsOsXZXFut2Znmed2WRti8bVXjwgq5cf5LNxW+OtCnjV974aTOX90ukZ2K823GMqVSVL3qq6jrgER9k8SuFRcrWfdmlisCmjEMUFHluvYSGCO2a1KN7yzgu6ZPI8vQDPPzRakJDhOtObOvuDxBkdh7MISo8hLjocL/8VP7YJ78QGRbKPUO7uB3FGK8E/R0xVWVXZu4RRWDtzizW784iJ7/ot69r1SiaLs1jOeuE5nRpEUvn5rG0b1qPyLDfF2XJKyhi3FuL+ceHqwgRYfigNm78SEFncdp+Ln3xZ1QhKjyE5g2iaN4gihYNomgRV/J1JM0bRNEsNoqIsNobkfzt2t18s2Y3fz3veJrF2tKfpm4I6uLw2cqd3Dd7GZk5v69Z1Cw2ki4tYrl2YBu6NI+lc4tYOjWrT73Iyv+oIsJCeOGavtwyLYX/+2AloSHC1TZnjs+9/P1G4qLDuW1IR3Zl5rAzM5ddB3NYuvUAO1flkFdQVOp7mtSP+K1oNI9zikeJ1w3rhRMXHX5E8T8WeQVFPPrxato1qceowXa50dQd3kyfIcC1QHtVfUREWgMtVHWhz9P5WJvGMVzYu6WnCDiPhvWqNzNmRFgIE4f35aY3U3jgvRWEinBF/1Y1lNgcbcueQ3yxehfjTu/ImFPalzquqhzIzmdnZg47M3PYddB5zsxh58Ecth/MYcnWA+w7lFfm+8dEhBIfHU6D6HDiY8KJj44gPiacuBKv46N/345ztmMiQhERps7bwqaMQ0walVSrrRVjqsublsNEoAg4A8+9hizgXaC/D3PVihOOa8BjF/Wo8feNDAvlpeH9GDs1mb+8t5zQEOHSfok1fh4Dk37aTHhICNcNLvsSnojQsF4EDetFcMJxDcp9n9yCQnZn5v5WOA5k53PwcD4HsvM4kJ3PgcP5HMzOZ9OeX3/bLqtFUiw8VIiLjiAzJ5/TOjdlSJdm1f5ZjalN3hSHgaraV0SWAKjqfhGxiecrERUeyqvXJTFmSjL3zF5GaIhwUZ8Et2MFlAPZebyTnM6w3i2rfS0/MiyUVo1iqjSdRU5+oVMonAKSnc/Bw78XkwPZ+RzOK+Dus7v45U1yYyriTXHIF5FQQAFEpCmeloSpRHGBGD15EXfPWkpIiHBhLxv8VFOmL0jjcH5hmZeTakNUeCgt4kJpEWc3mU3g8eYi6PPA+0AzEXkc+BGwQXBeio4I5fVRSfRv24i73l7KJ8t3uB0pIOQWFDL55y2c2rmprbtsjA94sxLcdOA+4AlgB3CRqr7j62CBJCYijEmj+tO3dTx3zFzCpyusQFTXh0u3k5GVy9hTrAeQMb5QaXFweidlAx8Bc4BDzj5TBfUiw3jj+gH0bhXP7TOW8PmqnW5HqrNUldfnbub4FrGcbAvlGOMT3lxW+gT42Hn+GtgEfOrLUIGqfmQYk6/vT/eEOG57azFfrd7ldqQ66Yf1e1i7K4sxp7S3G73G+Ig3l5V6qGpP57kTMADPVN7mGMRGhTP1hgF0Pa4Bt05fzLdrdrsdqc55be4mmsVG2s19Y3yoyqNyVHUxMNAHWYJGg6hwpt4wkC4tYrnpzRS+X5fhdqQ645cdmcxdv4eRg9vaoDJjfMibew53l3jcIyJvAdure2IRuV1E1ojIKhF5ssT+B0Rkg4isFZGh1T2Pv4qLDufNGwbQsVl9xk5NZu56KxDeeG3uZqLDQ7l2oN32MsaXvPnoFVviEYnn3sOw6pxURIY479FLVbsB/3b2d8UzHXg34FxgojPGIiDFx0QwfcxA2jepx5gpyfy8YY/bkfzarswc5izbxhVJicTH2DhMY3ypwuLg/GKOddaNflhVH1fV6aqaU83z3gKMV9VcAFUtvvA+DJipqrmquhnYgOceR8BqWM9TINo2rsfoKYuYt3Gv25H81pSft1BQpIw+2bqvGuNr5RYHEQlT1ULgJB+ctzNwiogsEJHvRaR4nqYEYGuJr0t39pWV70YRSRaR5IyMun1JpnH9SKaPHUirhjGMnryIhZv3uR3J72TnFTB9QRpDu7agTeN6bscxJuBV1HIonnV1qYjMEZERInJJ8aOyNxaRr0RkZRmPYXim7WgEDALuBWZJFfskquorqpqkqklNmzatyrf6pSb1I3lr7CBaxkcx6o2FJG+xAlHSO8npHDycz9hTrdVgTG3w5p5DFLAXz6ys5wMXOM8VUtWzVLV7GY8P8bQI3lOPhXjmamoCbANKzm+d6OwLCk1jI5kxdhAtGkQxctJCuwfhKCxSXv9xM31ax9OvTSO34xgTFCoqDs1E5G5gJbDCeV7lPK+s5nk/AIYAOGtSRwB78IzAvkpEIkWkHdCJ31swQaFZgyjeGjuIhIbRjHxjIe8tTnc7kuu+XL2TtH3Z3OjSBHvGBKOKikMoUN95xJZ4XfyojklAexFZCcwERjqtiFXALGA18BkwzrnvEVRaxEXxzs2DSWrTiLtnLeO/X69HVd2O5ZpX526mdaMYzunWwu0oxgSNiqbs3qGqj/jipKqaBwwv59jjwOO+OG9dEhcdzpTRA7j/3eU8/eU6th04zKMXdSc8NLgGfqWk7icldT8PXdCV0BCbKsOY2lJRcbD/iS6LCAvh6St6kdAwmv9+s4EdB3N44dq+1PdiPetA8drcTTSICuPyJFtq1ZjaVNHH0DNrLYUpl4jw53O6MP6SHvy4YQ9XvDSPXZnVHWZSN6TtzebzVTu5dlAb6gVRQTTGH5RbHFTV+lL6kasGtOb1kUmk7j3ExS/8xLpdWW5H8rlJP20mNEQYNbit21GMCTrBdQG7jju9SzPevulECoqUS1/8OaC7uh7MzmdW8lYu6NWS5g1sGU5japsVhzqme0Ic7487iePiohj5xkLeXxKYXV2nL0wlO6+QMSdb91Vj3GDFoQ5KiI/+ravrXW8vY8I3gdXVNa+giCk/b+Hkjk3o2rKB23GMCUpWHOqo4q6uF/dJ4N9frOOB91aQX1jkdqwa8dGy7ezKzGWMrQ9tjGusC0gdFhEWwjNX9CIhPpoJ3wZGV1dV5dW5m+jcvD6nda77c2YZU1dZy6GOExHuGdqFJ5yurle+XLe7uv60YS9rdmYx5mRbH9oYN1lxCBBXD2jNayOT2LznEJdM/LnOdnV9de4mmtSPZFgfWx/aGDdZcQggQ7o0Y9ZNJ5JXWOTp6rqxbnV1Xbcri+/XZTDyxDZEhgXsAoDG1AlWHAJM94Q43r918G/Tfn+wpO7MeP7a3E1EhYcwfFAbt6MYE/SsOASgxIYxzL55MP3aNOTOt5cy4Zv1FBb5d1fX3Vk5fLBkO5f1S6RhPVsf2hi3WXEIUHExnq6uw3q35N9frOPUJ79l4ncb2PtrrtvRyvTmvFTyi4q4wQa9GeMXrDgEsMiwUJ67sjcvXtuX1o1iePKztZz4xDfc9fZSUlL3+83AucN5hbw5P5WzTmhOuya2PrQx/qDudog3XhER/tDjOP7Q4zjW78pi2vxU3l28jfeXbKNbywaMGNSGC3u3JCbCvX8Ks1O2ciA7nxtPtVaDMf5C/OXTY3UkJSVpcnKy2zHqjEO5Bby/ZBvT5qeyZmcWsVFhXN6vFcMHtaZ90+ou8lc1hUXKmU9/R1xMBB/cOtjGNhhTi0QkRVWTyjpmLYcgVC8yjOGD2nDtwNYkp+5n6rxU3py/hUk/beaUTk0YPqgNZx7fjLBaWHXuq192sWVvNhOGdrHCYIwfseIQxESE/m0b0b9tI3ZnncCsRVuZviCNm95MoWVcFNcMbM2V/VvTNDbSZxlem7uJhPhozrX1oY3xK3ZD2gDQLDaK287oxNz7hvDyiH50aFaff3+xjsHjv+b2GUtYtGVfjd/AXrr1AIu27Gf0ye1qpZVijPGetRzMEcJCQxjarQVDu7VgY8avTJ+fxjspW/lo2XaObxHLVf1bcVx8NBGhIUSEOY/QEMKd7UhnX/G255iUecno1bmbiI0K48r+tj60Mf7GbkibSmXnFTBn6Xamzktl9Y7MY3qPUsUkTNi2/zBjT2nPA+edUMOJjTHesBvSplpiIsK4akBrruzfirR92WTlFJBXWER+QRF5hUXkFTiPwqOeC4rId17nFh65nVdQRP+2jRhr3VeN8UtWHIzXRIQ2jW2QmjHBwO4CGmOMKcWKgzHGmFKsOBhjjCnFioMxxphSrDgYY4wpxYqDMcaYUqw4GGOMKcWKgzHGmFICYvoMEckAUo/x25sAe2owTk3x11zgv9ksV9VYrqoJxFxtVLVpWQcCojhUh4gklze3iJv8NRf4bzbLVTWWq2qCLZddVjLGGFOKFQdjjDGlWHGAV9wOUA5/zQX+m81yVY3lqpqgyhX09xyMMcaUZi0HY4wxpVhxMMYYU0pQFwcROVdE1orIBhG53+08ACLSSkS+FZHVIrJKRP7kdqaSRCRURJaIyMduZykmIvEiMltE1ojILyJyotuZAETkLufvcKWIzBCRKJdyTBKR3SKyssS+RiLypYisd54b+kmup5y/x+Ui8r6IxNd2rvKylTj2ZxFREWniL7lE5Hbnz22ViDxZE+cK2uIgIqHAC8AfgK7A1SLS1d1UABQAf1bVrsAgYJyf5Cr2J+AXt0Mc5T/AZ6p6PNALP8gnIgnAHUCSqnYHQoGrXIozGTj3qH33A1+raifga2e7tk2mdK4vge6q2hNYBzxQ26EckymdDRFpBZwDpNV2IMdkjsolIkOAYUAvVe0G/LsmThS0xQEYAGxQ1U2qmgfMxPMH7CpV3aGqi53XWXh+0SW4m8pDRBKBPwKvuZ2lmIjEAacCrwOoap6qHnA11O/CgGgRCQNigO1uhFDVH4B9R+0eBkxxXk8BLqrNTFB2LlX9QlULnM35QGJt53JylPVnBvAscB/gSk+ecnLdAoxX1Vzna3bXxLmCuTgkAFtLbKfjJ7+Ei4lIW6APsMDlKMWew/Mfo8jlHCW1AzKAN5zLXa+JiOsLXavqNjyf4NKAHcBBVf3C3VRHaK6qO5zXO4HmboYpx2jgU7dDFBORYcA2VV3mdpajdAZOEZEFIvK9iPSviTcN5uLg10SkPvAucKeqZvpBnvOB3aqa4naWo4QBfYEXVbUPcAh3LpEcwbmGPwxP8WoJ1BOR4e6mKpt6+rP7VZ92Efkbnkus093OAiAiMcBfgX+4naUMYUAjPJeh7wVmiYhU902DuThsA1qV2E509rlORMLxFIbpqvqe23kcJwEXisgWPJfgzhCRae5GAjwtvnRVLW5dzcZTLNx2FrBZVTNUNR94DxjscqaSdonIcQDOc41ciqgJIjIKOB+4Vv1nIFYHPIV+mfN/IBFYLCItXE3lkQ68px4L8bTsq32zPJiLwyKgk4i0E5EIPDcL57icCafivw78oqrPuJ2nmKo+oKqJqtoWz5/VN6rq+idhVd0JbBWRLs6uM4HVLkYqlgYMEpEY5+/0TPzgRnkJc4CRzuuRwIcuZvmNiJyL59Llhaqa7XaeYqq6QlWbqWpb5/9AOtDX+ffntg+AIQAi0hmIoAZmjw3a4uDc9LoN+BzPf9pZqrrK3VSA5xP6CDyfzJc6j/PcDuXnbgemi8hyoDfwT3fjgNOSmQ0sBlbg+b/myvQLIjIDmAd0EZF0EbkBGA+cLSLr8bRyxvtJrglALPCl82//pdrOVUE215WTaxLQ3uneOhMYWRMtLps+wxhjTClB23IwxhhTPisOxhhjSrHiYIwxphQrDsYYY0qx4mCMMaYUKw7GlEFECkt0JV5a2ay9InKziFxXA+fd4sZsn8YczbqyGlMGEflVVeu7cN4teGZyrfYgJmOqw1oOxlSB88n+SRFZISILRaSjs/8hEbnHeX2HeNbjWC4iM519jUTkA2fffBHp6exvLCJfOPPwvwZIiXMNd86xVERedqaZN6ZWWHEwpmzRR11WurLEsYOq2gPPaN7nyvje+4E+zpoENzv7HgaWOPv+Ckx19j8I/OjMw/8+0BpARE4ArgROUtXeQCFwbU3+gMZUJMztAMb4qcPOL+WyzCjx/GwZx5fjmc7jAzzz3gCcDFwKoKrfOC2GBnjWorjE2f+JiOx3vv5MoB+wyJlgMxo/mhzPBD4rDsZUnZbzutgf8fzSvwD4m4j0OIZzCDBFVd1aCc0EObusZEzVXVnieV7JAyISArRS1W+BvwBxQH1gLs5lIRE5HdjjrNPxA3CNs/8PQPFazl8Dl4lIM+dYIxFp47sfyZgjWcvBmLJFi8jSEtufqWpxd9aGzgywucDVR31fKDDNWb5UgOdV9YCIPARMcr4vm9+ny34YmCEiq4CfcdYmVtXVIvJ/wBdOwckHxgGpNfxzGlMm68pqTBVYV1MTLOyykjHGmFKs5WCMMaYUazkYY4wpxYqDMcaYUqw4GGOMKcWKgzHGmFKsOBhjjCnl/wFgFPszt01P0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[-54.265199540551066,\n",
       "  -51.41179298793803,\n",
       "  -78.06507378606447,\n",
       "  -90.29524288176908,\n",
       "  -85.7387258064616,\n",
       "  -73.27162422827341,\n",
       "  57.017540460490764,\n",
       "  -60.712202858899005,\n",
       "  -59.811319210551176,\n",
       "  87.78764824535355,\n",
       "  91.07579338569064,\n",
       "  93.42535369887068,\n",
       "  93.36062403252072,\n",
       "  93.29436322239825,\n",
       "  92.53823177848717,\n",
       "  91.95509242823118,\n",
       "  91.39454923913817]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(total_trials=1, total_episodes=17, use_momentum=True, save_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88080b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.066567897796631s\n",
      "9.552883625030518s\n",
      "3.6586098670959473s\n",
      "3.533348321914673s\n",
      "33.615522384643555s\n",
      "3.713900566101074s\n",
      "371131.2107219696s\n",
      "3.637434720993042s\n"
     ]
    }
   ],
   "source": [
    "test(render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
