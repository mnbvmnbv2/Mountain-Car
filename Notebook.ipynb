{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593313f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  2\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "problem = \"MountainCarContinuous-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a25ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493ba423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c76273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89a8efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -885.5229244517011 * time used: 6.713388681411743\n",
      "Episode * 1 * Avg Reward is ==> -894.98262449271 * time used: 6.127302169799805\n",
      "Episode * 2 * Avg Reward is ==> -901.2013520327523 * time used: 6.306691408157349\n",
      "Episode * 3 * Avg Reward is ==> -764.5870986649907 * time used: 3.494612455368042\n",
      "Episode * 4 * Avg Reward is ==> -794.82979148691 * time used: 6.488022089004517\n",
      "Episode * 5 * Avg Reward is ==> -811.7307588217683 * time used: 6.559780597686768\n",
      "Episode * 6 * Avg Reward is ==> -823.537757671532 * time used: 6.704815864562988\n",
      "Episode * 7 * Avg Reward is ==> -835.4500024037736 * time used: 6.7825140953063965\n",
      "Episode * 8 * Avg Reward is ==> -841.8282986656116 * time used: 6.930941104888916\n",
      "Episode * 9 * Avg Reward is ==> -833.1801548997237 * time used: 7.088308334350586\n",
      "Episode * 10 * Avg Reward is ==> -842.5039052728154 * time used: 7.456094980239868\n",
      "Episode * 11 * Avg Reward is ==> -846.995760773707 * time used: 8.759736776351929\n",
      "Episode * 12 * Avg Reward is ==> -851.4081860075445 * time used: 7.608024597167969\n",
      "Episode * 13 * Avg Reward is ==> -854.9972476329896 * time used: 7.391784906387329\n",
      "Episode * 14 * Avg Reward is ==> -857.9059044068924 * time used: 7.2690277099609375\n",
      "Episode * 15 * Avg Reward is ==> -861.3664317912173 * time used: 7.017176151275635\n",
      "Episode * 16 * Avg Reward is ==> -857.4626077894343 * time used: 7.1601738929748535\n",
      "Episode * 17 * Avg Reward is ==> -858.837803936719 * time used: 7.086658239364624\n",
      "Episode * 18 * Avg Reward is ==> -860.1615950833653 * time used: 7.116963624954224\n",
      "Episode * 19 * Avg Reward is ==> -861.3100253109711 * time used: 7.806289434432983\n",
      "Episode * 20 * Avg Reward is ==> -861.8177819166982 * time used: 7.426127910614014\n",
      "Episode * 21 * Avg Reward is ==> -862.0834644321543 * time used: 7.960505962371826\n",
      "Episode * 22 * Avg Reward is ==> -860.8279385977881 * time used: 7.601923704147339\n",
      "Episode * 23 * Avg Reward is ==> -884.9037655633895 * time used: 8.059589385986328\n",
      "Episode * 24 * Avg Reward is ==> -882.8425798165763 * time used: 8.057260036468506\n",
      "Episode * 25 * Avg Reward is ==> -882.1614163490864 * time used: 8.107538938522339\n",
      "Episode * 26 * Avg Reward is ==> -882.3377251144695 * time used: 7.782652378082275\n",
      "Episode * 27 * Avg Reward is ==> -846.4007636334666 * time used: 2.9119455814361572\n",
      "Episode * 28 * Avg Reward is ==> -846.633982456013 * time used: 7.67233943939209\n",
      "Episode * 29 * Avg Reward is ==> -853.2767176594805 * time used: 7.540898323059082\n",
      "Episode * 30 * Avg Reward is ==> -851.9389800714074 * time used: 7.5071704387664795\n",
      "Episode * 31 * Avg Reward is ==> -851.6307794463326 * time used: 7.64627742767334\n",
      "Episode * 32 * Avg Reward is ==> -850.0301963419467 * time used: 7.836249113082886\n",
      "Episode * 33 * Avg Reward is ==> -850.0857434011732 * time used: 7.501482725143433\n",
      "Episode * 34 * Avg Reward is ==> -845.7281089038272 * time used: 7.358313083648682\n",
      "Episode * 35 * Avg Reward is ==> -844.7099398132337 * time used: 7.723268747329712\n",
      "Episode * 36 * Avg Reward is ==> -845.3588178228871 * time used: 7.835302352905273\n",
      "Episode * 37 * Avg Reward is ==> -844.6419722278986 * time used: 7.611595153808594\n",
      "Episode * 38 * Avg Reward is ==> -845.0806703443528 * time used: 7.578824758529663\n",
      "Episode * 39 * Avg Reward is ==> -843.7011274080924 * time used: 7.5576629638671875\n",
      "Episode * 40 * Avg Reward is ==> -840.9819266859516 * time used: 7.585433483123779\n",
      "Episode * 41 * Avg Reward is ==> -836.3556395379622 * time used: 7.653002977371216\n",
      "Episode * 42 * Avg Reward is ==> -837.1885177923447 * time used: 7.939406871795654\n",
      "Episode * 43 * Avg Reward is ==> -840.6079344336329 * time used: 7.758077144622803\n",
      "Episode * 44 * Avg Reward is ==> -820.0557427855492 * time used: 5.749112606048584\n",
      "Episode * 45 * Avg Reward is ==> -819.5934444212016 * time used: 7.558781862258911\n",
      "Episode * 46 * Avg Reward is ==> -820.8017680598081 * time used: 7.663357734680176\n",
      "Episode * 47 * Avg Reward is ==> -854.4633188363435 * time used: 7.839046001434326\n",
      "Episode * 48 * Avg Reward is ==> -850.9114893011019 * time used: 7.674095392227173\n",
      "Episode * 49 * Avg Reward is ==> -852.8483847757265 * time used: 8.448463678359985\n",
      "Episode * 50 * Avg Reward is ==> -851.6342696096365 * time used: 7.553426027297974\n",
      "Episode * 51 * Avg Reward is ==> -851.9138199386301 * time used: 7.480656385421753\n",
      "Episode * 52 * Avg Reward is ==> -853.1551545945979 * time used: 7.438866853713989\n",
      "Episode * 53 * Avg Reward is ==> -851.7793397292673 * time used: 7.522944927215576\n",
      "Episode * 54 * Avg Reward is ==> -851.8793837260221 * time used: 7.20010781288147\n",
      "Episode * 55 * Avg Reward is ==> -852.3667379426779 * time used: 7.265404462814331\n",
      "Episode * 56 * Avg Reward is ==> -856.2745833824462 * time used: 7.263102769851685\n",
      "Episode * 57 * Avg Reward is ==> -856.8562884660156 * time used: 7.394197225570679\n",
      "Episode * 58 * Avg Reward is ==> -856.6207812012202 * time used: 7.606701135635376\n",
      "Episode * 59 * Avg Reward is ==> -859.1142029991595 * time used: 8.227169752120972\n",
      "Episode * 60 * Avg Reward is ==> -862.0735482162384 * time used: 8.646637439727783\n",
      "Episode * 61 * Avg Reward is ==> -868.2914709055716 * time used: 8.78600811958313\n",
      "Episode * 62 * Avg Reward is ==> -866.7997162523737 * time used: 10.08967900276184\n",
      "Episode * 63 * Avg Reward is ==> -867.0015787882157 * time used: 9.457788944244385\n",
      "Episode * 64 * Avg Reward is ==> -890.5451045983422 * time used: 9.01407766342163\n",
      "Episode * 65 * Avg Reward is ==> -890.4276838984712 * time used: 7.601622581481934\n",
      "Episode * 66 * Avg Reward is ==> -885.7949736596198 * time used: 8.275224447250366\n",
      "Episode * 67 * Avg Reward is ==> -887.3378184337216 * time used: 7.51619291305542\n",
      "Episode * 68 * Avg Reward is ==> -887.8988823509386 * time used: 7.410048484802246\n",
      "Episode * 69 * Avg Reward is ==> -883.0088664390972 * time used: 7.9524664878845215\n",
      "Episode * 70 * Avg Reward is ==> -882.6423300856953 * time used: 8.079564571380615\n",
      "Episode * 71 * Avg Reward is ==> -879.0106890582208 * time used: 7.383191823959351\n",
      "Episode * 72 * Avg Reward is ==> -879.4122029536371 * time used: 7.323575735092163\n",
      "Episode * 73 * Avg Reward is ==> -880.6975063072362 * time used: 7.437500953674316\n",
      "Episode * 74 * Avg Reward is ==> -885.1803365464282 * time used: 7.537562608718872\n",
      "Episode * 75 * Avg Reward is ==> -883.096207530418 * time used: 7.524794578552246\n",
      "Episode * 76 * Avg Reward is ==> -885.0527837421465 * time used: 7.526262521743774\n",
      "Episode * 77 * Avg Reward is ==> -883.767503639196 * time used: 7.75078558921814\n",
      "Episode * 78 * Avg Reward is ==> -883.2063181656183 * time used: 7.795132398605347\n",
      "Episode * 79 * Avg Reward is ==> -882.0528666484348 * time used: 7.9408087730407715\n",
      "Episode * 80 * Avg Reward is ==> -883.3075417950198 * time used: 7.882484674453735\n",
      "Episode * 81 * Avg Reward is ==> -880.822982060183 * time used: 7.859206199645996\n",
      "Episode * 82 * Avg Reward is ==> -882.2534877785289 * time used: 8.059420585632324\n",
      "Episode * 83 * Avg Reward is ==> -881.0370763323244 * time used: 9.804901599884033\n",
      "Episode * 84 * Avg Reward is ==> -880.2514642415797 * time used: 10.058340549468994\n",
      "Episode * 85 * Avg Reward is ==> -880.9753126819803 * time used: 9.732730627059937\n",
      "Episode * 86 * Avg Reward is ==> -878.6072796198644 * time used: 8.474257230758667\n",
      "Episode * 87 * Avg Reward is ==> -877.985467294835 * time used: 8.33156681060791\n",
      "Episode * 88 * Avg Reward is ==> -880.6624786854729 * time used: 7.543284893035889\n",
      "Episode * 89 * Avg Reward is ==> -884.3431700955218 * time used: 7.796551704406738\n",
      "Episode * 90 * Avg Reward is ==> -882.2704358561253 * time used: 8.16949987411499\n",
      "Episode * 91 * Avg Reward is ==> -885.0480020464371 * time used: 8.214042901992798\n",
      "Episode * 92 * Avg Reward is ==> -883.7814794321597 * time used: 8.533025741577148\n",
      "Episode * 93 * Avg Reward is ==> -882.7859341435812 * time used: 8.334893703460693\n",
      "Episode * 94 * Avg Reward is ==> -881.5824249625042 * time used: 9.503836393356323\n",
      "Episode * 95 * Avg Reward is ==> -882.5068052707177 * time used: 8.149645328521729\n",
      "Episode * 96 * Avg Reward is ==> -877.1906964578695 * time used: 8.3107328414917\n",
      "Episode * 97 * Avg Reward is ==> -878.4520738296089 * time used: 9.378738641738892\n",
      "Episode * 98 * Avg Reward is ==> -879.8378614310637 * time used: 8.762276887893677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 99 * Avg Reward is ==> -881.1115963458885 * time used: 7.887542724609375\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "for ep in range(total_episodes):\n",
    "    \n",
    "    before = time.time()\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "    # The furthest right it has been\n",
    "    right_max = env.state[0]\n",
    "\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # https://medium.com/reinforcement-learning-w-policy-gradients/mountaincarcontinous-cheating-8446b09647ba\n",
    "        true_state = np.abs(np.cos(np.pi/3.) + state[0])\n",
    "        reward += -(1. - true_state)\n",
    "        \n",
    "        # Update the furthest right it has been\n",
    "        # if state[0] > right_max:\n",
    "        #    right_max = state[0]\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "        \n",
    "    # add as reward for going further right, but make sure it is above 0. Add +0.4 to make it get value in the beginning too\n",
    "    #buffer.record((prev_state, action, max(0, (right_max + 0.4) * 30), state))\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    used_time = time.time() - before\n",
    "    \n",
    "    # Mean of last 20 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-20:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {} * time used: {}\".format(ep, avg_reward, used_time))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"Weights/car_actor.h5\")\n",
    "critic_model.save_weights(\"Weights/car_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"Weights/car_target_actor.h5\")\n",
    "target_critic.save_weights(\"Weights/car_target_critic.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb550c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model = get_actor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4fea408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the weights equal initially\n",
    "actor_model.load_weights(\"/Users/Ferdi/My Drive/ColabNotebooks/Weights/car_actor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a75429",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_state = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "\n",
    "    tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "    action = policy(tf_prev_state, ou_noise)\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "    prev_state = state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
