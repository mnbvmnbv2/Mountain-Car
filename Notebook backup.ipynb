{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7764110b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\utils\\image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593313f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  2\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "problem = \"MountainCarContinuous-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "# This is needed to get the input size for the NN\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "# This is needed to clip the actions within the legal boundaries\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a25ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a noise used in the paper that introduced DDPG https://arxiv.org/pdf/1509.02971v6.pdf\n",
    "# From what I understand, we can also use Gaussian or other noise without much difference,\n",
    "# I left the noise as is\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493ba423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its important to make sure we only sample from used buffer space\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Makes a record of the outputted (s,a,r,s') obervation tuple\n",
    "    def record(self, obs_tuple):\n",
    "        # Reuse the same buffer replacing old entries\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f235a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor(layer1=400, layer2=300):\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Multiply to fill the whole action space which should be equal around 0\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "def get_critic(layer1=400, layer2=300):\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(layer1, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(layer2, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Make it into a keras model\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c76273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d09a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "            actor_lr=0.001, gamma=0.99, tau=0.005):\n",
    "        \n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.std_dev = std_dev\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.actor_model = get_actor(layer1=400, layer2=300)\n",
    "        self.critic_model = get_critic(layer1=400, layer2=300)\n",
    "\n",
    "        self.target_actor = get_actor(layer1=400, layer2=300)\n",
    "        self.target_critic = get_critic(layer1=400, layer2=300)\n",
    "        \n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "        \n",
    "        # Making the weights equal initially\n",
    "        self.target_actor.set_weights(self.actor_model.get_weights())\n",
    "        self.target_critic.set_weights(self.critic_model.get_weights())\n",
    "        \n",
    "        self.buffer = Buffer(buffer_capacity, batch_size)\n",
    "        \n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "    \n",
    "    # Move the update and learn function from buffer to Agent to \"decrease\" scope\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor_model(state_batch, training=True)\n",
    "            critic_value = self.critic_model([state_batch, actions], training=True)\n",
    "\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Sample only valid data\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "    def policy(self, state, noise_object, use_noise=True):\n",
    "        # For doing actions without added noise\n",
    "        if not use_noise:     \n",
    "            sampled_actions = tf.squeeze(actor_model(state)).numpy()\n",
    "            legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]\n",
    "        else:\n",
    "            sampled_actions = tf.squeeze(self.actor_model(state))\n",
    "            noise = noise_object()\n",
    "            # Adding noise to action\n",
    "            sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "            # We make sure action is within bounds\n",
    "            legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "            return [np.squeeze(legal_action)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89a8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This updates the weights in a slow manner which keeps stability\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f9f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(total_trials=3, total_episodes=100, use_curve=False, curve_scale=1, use_momentum=False):\n",
    "    # To store reward history of each episode\n",
    "    ep_reward_list = []\n",
    "    # To store average reward history of last few episodes\n",
    "    avg_reward_list = []\n",
    "    # To separate assisted reward structures from the \"true\"\n",
    "    true_reward_list = []\n",
    "    true_avg_reward_list = []\n",
    "    \n",
    "    for trial in range(total_trials):\n",
    "\n",
    "        # add sublists for each trial\n",
    "        avg_reward_list.append([])\n",
    "        ep_reward_list.append([])\n",
    "        \n",
    "        true_reward_list.append([])\n",
    "        true_avg_reward_list.append([])\n",
    "        \n",
    "        agent = Agent(buffer_capacity=50000, batch_size=64, std_dev=0.2, critic_lr=0.002,\n",
    "                actor_lr=0.001, gamma=0.99, tau=0.005)\n",
    "\n",
    "        for ep in range(total_episodes):\n",
    "            # Used for time benchmarking\n",
    "            before = time.time()\n",
    "\n",
    "            prev_state = env.reset()\n",
    "            episodic_reward = 0\n",
    "            true_reward = 0\n",
    "\n",
    "            while True:\n",
    "                tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "                action = agent.policy(tf_prev_state, agent.ou_noise)\n",
    "                # Recieve state and reward from environment.\n",
    "                state, reward, done, info = env.step(action)\n",
    "                \n",
    "                # Add this before eventual reward modification\n",
    "                true_reward += reward\n",
    "                \n",
    "                if use_curve:\n",
    "                    # https://medium.com/reinforcement-learning-w-policy-gradients/mountaincarcontinous-cheating-8446b09647ba\n",
    "                    # true_state = np.abs(np.cos(np.pi/3.) + state[0])\n",
    "                    # reward += -(1. - true_state)\n",
    "                    \n",
    "                    # I am a bit unsure how the curve is calculated (it just says sinusoidal on the webpage)\n",
    "                    # I base my attempt at the bottom being -0.5 and top right being 0.5\n",
    "                    # so the below code is my attempt at this type of reward\n",
    "                    reward += (np.sin(state[0] * np.pi) + 1)/curve_scale\n",
    "                    # it adds 0 at the bottom and 2 at the top of the curve\n",
    "                    # adds curve scale as the reward could be too high\n",
    "                    \n",
    "                if use_momentum:\n",
    "                    # having 0 speed and being in 0,5 (which i guess is the bottom) should incur max penalty\n",
    "                    # having max speed and beging in 0,5 should not give any penalty\n",
    "                    # the rest of the penalties should be smooth around the bottom with regards to the speed\n",
    "                    reward += 1\n",
    "\n",
    "                agent.buffer.record((prev_state, action, reward, state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                agent.learn()\n",
    "                update_target(agent.target_actor.variables, agent.actor_model.variables, agent.tau)\n",
    "                update_target(agent.target_critic.variables, agent.critic_model.variables, agent.tau)\n",
    "\n",
    "                # End this episode if en episode is done\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "\n",
    "            ep_reward_list[trial].append(episodic_reward)\n",
    "            \n",
    "            true_reward_list[trial].append(true_reward)\n",
    "            \n",
    "            true_avg_reward = np.mean(true_reward_list[trial][-40:])\n",
    "            true_avg_reward_list[trial].append(true_avg_reward)\n",
    "\n",
    "            # Mean of last 40 episodes\n",
    "            avg_reward = np.mean(ep_reward_list[trial][-40:])\n",
    "            print(\"Episode * {} * Avg Reward is ==> {} * true_avg_reward: {} * time used: {}\"\n",
    "                  .format(ep, avg_reward, true_avg_reward, (time.time() - before)))\n",
    "            avg_reward_list[trial].append(avg_reward)\n",
    "\n",
    "    # Plotting graph\n",
    "    for idx, p in enumerate(true_avg_reward_list):\n",
    "        plt.plot(p, label=str(idx))\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"True Avg. Epsiodic Reward (40)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4268c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> 181.568018548838 * true_avg_reward: -33.529075237328165 * time used: 14.68755578994751\n",
      "Episode * 1 * Avg Reward is ==> 330.25768983445727 * true_avg_reward: -45.96231426148524 * time used: 13.263930320739746\n",
      "Episode * 2 * Avg Reward is ==> 402.2328215810962 * true_avg_reward: -8.011901478765244 * time used: 12.340706586837769\n",
      "Episode * 3 * Avg Reward is ==> 348.21795379165945 * true_avg_reward: 16.255517530056473 * time used: 2.243436098098755\n",
      "Episode * 4 * Avg Reward is ==> 323.89163902420705 * true_avg_reward: 28.056234718559104 * time used: 3.433723211288452\n",
      "Episode * 5 * Avg Reward is ==> 296.6540851515933 * true_avg_reward: 38.38006228537777 * time used: 1.4997689723968506\n",
      "Episode * 6 * Avg Reward is ==> 289.9464516487962 * true_avg_reward: 35.79014952655701 * time used: 10.92523741722107\n",
      "Episode * 7 * Avg Reward is ==> 271.9446996835301 * true_avg_reward: 42.957523256967 * time used: 1.3322055339813232\n",
      "Episode * 8 * Avg Reward is ==> 262.2313629132275 * true_avg_reward: 47.982420430257385 * time used: 2.307527542114258\n",
      "Episode * 9 * Avg Reward is ==> 253.0882215811858 * true_avg_reward: 51.89458114861287 * time used: 2.3051116466522217\n",
      "Episode * 10 * Avg Reward is ==> 247.87441486586883 * true_avg_reward: 54.60490255617765 * time used: 3.072305917739868\n",
      "Episode * 11 * Avg Reward is ==> 263.6817823131579 * true_avg_reward: 56.23492858207927 * time used: 6.421372413635254\n",
      "Episode * 12 * Avg Reward is ==> 256.91072967464333 * true_avg_reward: 58.83736065077184 * time used: 2.0463860034942627\n",
      "Episode * 13 * Avg Reward is ==> 250.654940540543 * true_avg_reward: 60.98572025553675 * time used: 1.7885868549346924\n",
      "Episode * 14 * Avg Reward is ==> 245.24077371923707 * true_avg_reward: 62.91205853831164 * time used: 1.553201675415039\n",
      "Episode * 15 * Avg Reward is ==> 239.5294764842848 * true_avg_reward: 64.8117981277839 * time used: 1.4332301616668701\n",
      "Episode * 16 * Avg Reward is ==> 239.74431478339423 * true_avg_reward: 63.97297730563066 * time used: 8.486696481704712\n",
      "Episode * 17 * Avg Reward is ==> 236.29361620268435 * true_avg_reward: 65.51852939300396 * time used: 1.55961275100708\n",
      "Episode * 18 * Avg Reward is ==> 232.47368852568832 * true_avg_reward: 66.90128381081638 * time used: 1.3436024188995361\n",
      "Episode * 19 * Avg Reward is ==> 229.02011870018765 * true_avg_reward: 68.14682870238401 * time used: 1.3385725021362305\n",
      "Episode * 20 * Avg Reward is ==> 226.23091915236395 * true_avg_reward: 69.30333186264211 * time used: 1.3615353107452393\n",
      "Episode * 21 * Avg Reward is ==> 222.92732946348434 * true_avg_reward: 70.30742853918169 * time used: 1.2499644756317139\n",
      "Episode * 22 * Avg Reward is ==> 221.58287428726112 * true_avg_reward: 71.26658389123403 * time used: 1.4889228343963623\n",
      "Episode * 23 * Avg Reward is ==> 220.1073275841203 * true_avg_reward: 72.15265054634462 * time used: 1.466442346572876\n",
      "Episode * 24 * Avg Reward is ==> 218.44041487535043 * true_avg_reward: 73.00705770051898 * time used: 1.1994893550872803\n",
      "Episode * 25 * Avg Reward is ==> 215.85268442503653 * true_avg_reward: 73.74320918675103 * time used: 1.2729568481445312\n",
      "Episode * 26 * Avg Reward is ==> 213.83438828811964 * true_avg_reward: 74.43670213733127 * time used: 1.1701781749725342\n",
      "Episode * 27 * Avg Reward is ==> 212.10293637715088 * true_avg_reward: 75.05138032827719 * time used: 1.2792489528656006\n",
      "Episode * 28 * Avg Reward is ==> 209.96423540869733 * true_avg_reward: 75.6413886937933 * time used: 1.4446065425872803\n",
      "Episode * 29 * Avg Reward is ==> 207.89017105206705 * true_avg_reward: 76.18518719443907 * time used: 1.2531473636627197\n",
      "Episode * 30 * Avg Reward is ==> 206.3118536522168 * true_avg_reward: 76.71699560016962 * time used: 1.2366127967834473\n",
      "Episode * 31 * Avg Reward is ==> 204.77935429381412 * true_avg_reward: 77.19392998793472 * time used: 1.1593754291534424\n",
      "Episode * 32 * Avg Reward is ==> 203.53930799995393 * true_avg_reward: 77.65553819043545 * time used: 1.0338759422302246\n",
      "Episode * 33 * Avg Reward is ==> 202.63792498917874 * true_avg_reward: 78.12704513310611 * time used: 1.3663346767425537\n",
      "Episode * 34 * Avg Reward is ==> 203.3071803417729 * true_avg_reward: 78.44233915860264 * time used: 2.5125272274017334\n",
      "Episode * 35 * Avg Reward is ==> 201.959023314642 * true_avg_reward: 78.85825740034126 * time used: 1.404811143875122\n",
      "Episode * 36 * Avg Reward is ==> 201.1527906957689 * true_avg_reward: 79.24033767320344 * time used: 1.351858377456665\n",
      "Episode * 37 * Avg Reward is ==> 199.71878708286118 * true_avg_reward: 79.6048703331503 * time used: 1.169825792312622\n",
      "Episode * 38 * Avg Reward is ==> 198.97974157258207 * true_avg_reward: 79.94159279071518 * time used: 1.1672987937927246\n",
      "Episode * 39 * Avg Reward is ==> 198.23964446573763 * true_avg_reward: 80.29140459271454 * time used: 1.199519157409668\n",
      "Episode * 40 * Avg Reward is ==> 197.73026343089725 * true_avg_reward: 83.47834246132444 * time used: 1.2363755702972412\n",
      "Episode * 41 * Avg Reward is ==> 189.60155769096298 * true_avg_reward: 87.27707015323601 * time used: 1.3013944625854492\n",
      "Episode * 42 * Avg Reward is ==> 179.99934336019876 * true_avg_reward: 87.87543728667693 * time used: 1.0789687633514404\n",
      "Episode * 43 * Avg Reward is ==> 178.7037433603659 * true_avg_reward: 87.98009987167765 * time used: 0.8656277656555176\n",
      "Episode * 44 * Avg Reward is ==> 176.46478482399894 * true_avg_reward: 88.42246409177065 * time used: 1.0303871631622314\n",
      "Episode * 45 * Avg Reward is ==> 175.88579114766281 * true_avg_reward: 88.5175506311342 * time used: 1.1330292224884033\n",
      "Episode * 46 * Avg Reward is ==> 172.95682738708612 * true_avg_reward: 90.36707104735646 * time used: 0.8906557559967041\n",
      "Episode * 47 * Avg Reward is ==> 172.75392648546804 * true_avg_reward: 90.3733506424679 * time used: 1.0890517234802246\n",
      "Episode * 48 * Avg Reward is ==> 171.4359620888394 * true_avg_reward: 90.52014660493151 * time used: 1.101400375366211\n",
      "Episode * 49 * Avg Reward is ==> 170.46892168328947 * true_avg_reward: 90.68730399644836 * time used: 1.1270983219146729\n",
      "Episode * 50 * Avg Reward is ==> 168.92542313526934 * true_avg_reward: 90.9975677313599 * time used: 1.0758657455444336\n",
      "Episode * 51 * Avg Reward is ==> 161.42630874651167 * true_avg_reward: 91.49739590776572 * time used: 1.2269728183746338\n",
      "Episode * 52 * Avg Reward is ==> 160.41919630207377 * true_avg_reward: 91.5983344797914 * time used: 1.0395474433898926\n",
      "Episode * 53 * Avg Reward is ==> 159.53497064946288 * true_avg_reward: 91.71181986130304 * time used: 1.1033382415771484\n",
      "Episode * 54 * Avg Reward is ==> 159.49784835403173 * true_avg_reward: 91.69639838566236 * time used: 1.9319438934326172\n",
      "Episode * 55 * Avg Reward is ==> 158.91330179252762 * true_avg_reward: 91.69516791243043 * time used: 1.1206269264221191\n",
      "Episode * 56 * Avg Reward is ==> 156.1522460640102 * true_avg_reward: 92.77804453549801 * time used: 0.9570176601409912\n",
      "Episode * 57 * Avg Reward is ==> 155.1541031012163 * true_avg_reward: 92.83273938868459 * time used: 0.9448807239532471\n",
      "Episode * 58 * Avg Reward is ==> 154.34478087112817 * true_avg_reward: 92.87402988873818 * time used: 0.9184350967407227\n",
      "Episode * 59 * Avg Reward is ==> 154.38011879959123 * true_avg_reward: 92.81451230179114 * time used: 1.5778989791870117\n",
      "Episode * 60 * Avg Reward is ==> 153.50265894555602 * true_avg_reward: 92.83261821702227 * time used: 0.9536974430084229\n",
      "Episode * 61 * Avg Reward is ==> 153.03791772426513 * true_avg_reward: 92.8889450340849 * time used: 0.8758032321929932\n",
      "Episode * 62 * Avg Reward is ==> 151.54229501948262 * true_avg_reward: 92.93321254125591 * time used: 0.8350870609283447\n",
      "Episode * 63 * Avg Reward is ==> 150.15292005783087 * true_avg_reward: 92.97597860697881 * time used: 0.8593747615814209\n",
      "Episode * 64 * Avg Reward is ==> 150.20013238530743 * true_avg_reward: 92.85979740934457 * time used: 1.8668076992034912\n",
      "Episode * 65 * Avg Reward is ==> 149.8819148976667 * true_avg_reward: 92.90240989734502 * time used: 1.2251369953155518\n",
      "Episode * 66 * Avg Reward is ==> 149.08825449081547 * true_avg_reward: 92.95121773430607 * time used: 1.0369281768798828\n",
      "Episode * 67 * Avg Reward is ==> 148.48806998531828 * true_avg_reward: 93.02444593986397 * time used: 1.176863431930542\n",
      "Episode * 68 * Avg Reward is ==> 147.95674722580938 * true_avg_reward: 93.06128984505588 * time used: 1.1374640464782715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 69 * Avg Reward is ==> 147.6535494825455 * true_avg_reward: 93.10049331051206 * time used: 0.9742105007171631\n",
      "Episode * 70 * Avg Reward is ==> 146.9376454566369 * true_avg_reward: 93.12586843873684 * time used: 0.8882861137390137\n",
      "Episode * 71 * Avg Reward is ==> 147.50822471343147 * true_avg_reward: 93.02543385225775 * time used: 1.6178147792816162\n",
      "Episode * 72 * Avg Reward is ==> 146.6328402633463 * true_avg_reward: 93.05646319716978 * time used: 0.8503775596618652\n",
      "Episode * 73 * Avg Reward is ==> 146.26485179091137 * true_avg_reward: 92.97474084775817 * time used: 1.30018949508667\n",
      "Episode * 74 * Avg Reward is ==> 144.81045354368536 * true_avg_reward: 92.9647618344864 * time used: 1.439666509628296\n",
      "Episode * 75 * Avg Reward is ==> 145.1069531403783 * true_avg_reward: 92.87359886789409 * time used: 1.6731688976287842\n",
      "Episode * 76 * Avg Reward is ==> 144.84943075512655 * true_avg_reward: 92.893401624598 * time used: 1.8267395496368408\n",
      "Episode * 77 * Avg Reward is ==> 146.48192381485387 * true_avg_reward: 92.90293213271535 * time used: 2.5369606018066406\n",
      "Episode * 78 * Avg Reward is ==> 148.617084978542 * true_avg_reward: 92.89786406095166 * time used: 5.405238151550293\n",
      "Episode * 79 * Avg Reward is ==> 149.26968743219126 * true_avg_reward: 92.87218137774468 * time used: 2.0450828075408936\n",
      "Episode * 80 * Avg Reward is ==> 150.50414299544084 * true_avg_reward: 92.80565988305345 * time used: 2.037198781967163\n",
      "Episode * 81 * Avg Reward is ==> 149.94832341609734 * true_avg_reward: 92.81903402472716 * time used: 0.9570729732513428\n",
      "Episode * 82 * Avg Reward is ==> 150.510341844725 * true_avg_reward: 92.78425639274315 * time used: 1.467381477355957\n",
      "Episode * 83 * Avg Reward is ==> 152.62006108105783 * true_avg_reward: 92.59890740762452 * time used: 2.883049726486206\n",
      "Episode * 84 * Avg Reward is ==> 154.23121272988425 * true_avg_reward: 92.44245836867341 * time used: 2.7272796630859375\n",
      "Episode * 85 * Avg Reward is ==> 155.49419353477933 * true_avg_reward: 92.30935150601118 * time used: 1.5497024059295654\n",
      "Episode * 86 * Avg Reward is ==> 155.85262066064166 * true_avg_reward: 92.28546552534243 * time used: 0.9507191181182861\n",
      "Episode * 87 * Avg Reward is ==> 156.1529878663987 * true_avg_reward: 92.28387468553517 * time used: 1.0189414024353027\n",
      "Episode * 88 * Avg Reward is ==> 157.28751549235838 * true_avg_reward: 92.24795329738402 * time used: 1.6115405559539795\n",
      "Episode * 89 * Avg Reward is ==> 158.4305030738215 * true_avg_reward: 92.24008539783426 * time used: 1.892749309539795\n",
      "Episode * 90 * Avg Reward is ==> 165.7880457303255 * true_avg_reward: 92.03374115974978 * time used: 5.714660406112671\n",
      "Episode * 91 * Avg Reward is ==> 166.81567932292288 * true_avg_reward: 92.00950766733627 * time used: 2.1638219356536865\n",
      "Episode * 92 * Avg Reward is ==> 167.0094659037318 * true_avg_reward: 92.03994894648079 * time used: 1.2265326976776123\n",
      "Episode * 93 * Avg Reward is ==> 167.35847377214682 * true_avg_reward: 92.09620851521836 * time used: 0.967320442199707\n",
      "Episode * 94 * Avg Reward is ==> 168.1335744349632 * true_avg_reward: 92.18922814697729 * time used: 1.954042673110962\n",
      "Episode * 95 * Avg Reward is ==> 169.36336570816016 * true_avg_reward: 92.19961042862032 * time used: 1.885993480682373\n",
      "Episode * 96 * Avg Reward is ==> 169.59600636592927 * true_avg_reward: 92.2384345165381 * time used: 0.8517670631408691\n",
      "Episode * 97 * Avg Reward is ==> 169.9395797435408 * true_avg_reward: 92.24709314131971 * time used: 1.0576019287109375\n",
      "Episode * 98 * Avg Reward is ==> 171.11223903983696 * true_avg_reward: 92.20864205594899 * time used: 1.989353895187378\n",
      "Episode * 99 * Avg Reward is ==> 170.78023540931886 * true_avg_reward: 92.36541064220572 * time used: 1.2496113777160645\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run(total_trials=1, total_episodes=100, use_curve=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c39bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rest is previous code used to load weights and render. Can be reimplemented in next version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb550c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_model = get_actor()\n",
    "# actor_model.load_weights(\"/Users/Ferdi/My Drive/ColabNotebooks/Weights/car_actor.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a75429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_state = env.reset()\n",
    "# while True:\n",
    "#     env.render()\n",
    "\n",
    "#     tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "#     action = policy(tf_prev_state, ou_noise)\n",
    "#     state, _, done, _ = env.step(action)\n",
    "\n",
    "#     if done:\n",
    "#         break\n",
    "\n",
    "#     prev_state = state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
